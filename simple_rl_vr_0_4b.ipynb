{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyfam_SmD4MN",
        "outputId": "c8ec7b74-5bcf-4479-ada0-01b4fc48220d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vllm\n",
            "  Downloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Downloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl (294.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.1/294.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vllm, unsloth\n",
            "Successfully installed unsloth-2025.3.19 vllm-0.8.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps unsloth vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T4TkctkFCOp",
        "outputId": "e1c95223-f556-4e93-811d-9fcda404c4b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
            "  Cloning https://github.com/huggingface/transformers (to revision v4.49.0-Gemma-3) to /tmp/pip-req-build-gsgnhdox\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-gsgnhdox\n",
            "  Running command git checkout -q 367bab469b0ef32017e2a0a0a5dbac5d36002f03\n",
            "  Resolved https://github.com/huggingface/transformers to commit 367bab469b0ef32017e2a0a0a5dbac5d36002f03\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10936469 sha256=e6c29f74f98053319e24266b1e43876ea1f9a673da93ac80f1e0cf11f61974a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_566zz6v/wheels/53/15/d5/d63b866c641d8863f9cd29a4cc7a5efc38476c3aae8247c195\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.50.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sS6S7i6L7e1e",
        "outputId": "202ba559-1ca6-4cf4-8924-ec23ebe5ee46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.19)\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting git+https://github.com/huggingface/trl@main\n",
            "  Cloning https://github.com/huggingface/trl (to revision main) to /tmp/pip-req-build-0649n4a5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl /tmp/pip-req-build-0649n4a5\n",
            "  Resolved https://github.com/huggingface/trl to commit a9b27f82d67c366f421638941a05f86363b255b1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.18.0.dev0) (1.5.2)\n",
            "Collecting datasets>=3.0.0 (from trl==0.18.0.dev0)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.18.0.dev0) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.18.0.dev0) (4.50.0.dev0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.18.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.18.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.18.0.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl==0.18.0.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.18.0.dev0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.18.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.18.0.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets>=3.0.0->trl==0.18.0.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl==0.18.0.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl==0.18.0.dev0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.18.0.dev0) (3.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl==0.18.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl==0.18.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.18.0.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.18.0.dev0) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl==0.18.0.dev0) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl==0.18.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.18.0.dev0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.18.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.18.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.18.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.18.0.dev0) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.18.0.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.18.0.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.18.0.dev0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.18.0.dev0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl==0.18.0.dev0) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: trl\n",
            "  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trl: filename=trl-0.18.0.dev0-py3-none-any.whl size=348114 sha256=ebef3ef4a60683352ef8b06f0fa2039c30e1156fc702282ba62485fcfbf7c19a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p8o9ibfo/wheels/7e/5f/98/877dc5b103e21ee1c43ca8ca1133a0a41f32be9db4c672ac44\n",
            "Successfully built trl\n",
            "Installing collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.3.19 requires bitsandbytes, which is not installed.\n",
            "unsloth 2025.3.19 requires hf_transfer, which is not installed.\n",
            "unsloth 2025.3.19 requires tyro, which is not installed.\n",
            "unsloth 2025.3.19 requires unsloth_zoo>=2025.3.17, which is not installed.\n",
            "unsloth 2025.3.19 requires xformers>=0.0.27.post2, which is not installed.\n",
            "unsloth 2025.3.19 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.3.19 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.18.0.dev0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.18.0.dev0 xxhash-3.5.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate>=0.28 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 unsloth_zoo-2025.3.17 xformers-0.0.29.post3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.3.19 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.3.17 requires tyro, which is not installed.\n",
            "unsloth 2025.3.19 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.3.19 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.18.0.dev0 which is incompatible.\n",
            "unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.18.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hf_transfer-0.1.9\n",
            "Ignoring six: markers 'python_version > \"3.11\"' don't match your environment\n",
            "Ignoring setuptools: markers 'python_version > \"3.11\"' don't match your environment\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 6)) (4.67.1)\n",
            "Collecting blake3 (from -r vllm_requirements.txt (line 7))\n",
            "  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 8)) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 10)) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 11)) (5.29.4)\n",
            "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 13)) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 14)) (1.75.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 15)) (2.11.3)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 16)) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 17)) (11.1.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from -r vllm_requirements.txt (line 18))\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from -r vllm_requirements.txt (line 19))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from -r vllm_requirements.txt (line 20))\n",
            "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from -r vllm_requirements.txt (line 21))\n",
            "  Downloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting outlines==0.1.11 (from -r vllm_requirements.txt (line 22))\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from -r vllm_requirements.txt (line 23))\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from -r vllm_requirements.txt (line 24))\n",
            "  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 25)) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 26)) (3.18.0)\n",
            "Collecting partial-json-parser (from -r vllm_requirements.txt (line 27))\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from -r vllm_requirements.txt (line 28))\n",
            "  Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from -r vllm_requirements.txt (line 29))\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from -r vllm_requirements.txt (line 30))\n",
            "  Downloading gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 31)) (8.6.1)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->-r vllm_requirements.txt (line 32))\n",
            "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 33)) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 34)) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 37)) (0.8.1)\n",
            "Collecting compressed-tensors==0.9.3 (from -r vllm_requirements.txt (line 38))\n",
            "  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from -r vllm_requirements.txt (line 39))\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 40)) (3.1.1)\n",
            "Collecting watchfiles (from -r vllm_requirements.txt (line 41))\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from -r vllm_requirements.txt (line 42))\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r vllm_requirements.txt (line 43)) (1.14.1)\n",
            "Collecting ninja (from -r vllm_requirements.txt (line 44))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from -r vllm_requirements.txt (line 45))\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from -r vllm_requirements.txt (line 46))\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from -r vllm_requirements.txt (line 47))\n",
            "  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from -r vllm_requirements.txt (line 48))\n",
            "  Downloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting interegular (from outlines==0.1.11->-r vllm_requirements.txt (line 22))\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->-r vllm_requirements.txt (line 22))\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->-r vllm_requirements.txt (line 22))\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->-r vllm_requirements.txt (line 22))\n",
            "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->-r vllm_requirements.txt (line 22)) (2.6.0+cu124)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->-r vllm_requirements.txt (line 22))\n",
            "  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.18->-r vllm_requirements.txt (line 24)) (4.50.0.dev0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.18->-r vllm_requirements.txt (line 24)) (3.2.0)\n",
            "Collecting astor (from depyf==0.18.0->-r vllm_requirements.txt (line 39))\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->-r vllm_requirements.txt (line 39)) (0.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->-r vllm_requirements.txt (line 5)) (2025.1.31)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9)) (24.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->-r vllm_requirements.txt (line 13)) (1.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->-r vllm_requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->-r vllm_requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->-r vllm_requirements.txt (line 15)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->-r vllm_requirements.txt (line 15)) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->-r vllm_requirements.txt (line 19)) (2024.11.6)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->-r vllm_requirements.txt (line 31)) (3.21.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 45))\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 46)) (1.2.18)\n",
            "Collecting importlib_metadata (from -r vllm_requirements.txt (line 31))\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 47))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 47))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 47)) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 47)) (1.71.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 47))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 47))\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from -r vllm_requirements.txt (line 11))\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->-r vllm_requirements.txt (line 9))\n",
            "  Downloading hf_xet-1.0.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->-r vllm_requirements.txt (line 46)) (1.17.2)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.15.2)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading rich_toolkit-0.14.3-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.0->-r vllm_requirements.txt (line 14)) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.0->-r vllm_requirements.txt (line 14)) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.24.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->outlines==0.1.11->-r vllm_requirements.txt (line 22)) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->xgrammar==0.1.18->-r vllm_requirements.txt (line 24)) (0.5.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (8.1.8)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12))\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (15.0.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->-r vllm_requirements.txt (line 12)) (0.1.2)\n",
            "Downloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.16.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.3-py3-none-any.whl (5.4 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading hf_xet-1.0.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.3-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blake3, uvloop, uvicorn, pyzmq, python-multipart, python-json-logger, python-dotenv, pycountry, protobuf, partial-json-parser, opentelemetry-semantic-conventions-ai, ninja, msgspec, llguidance, lark, interegular, importlib_metadata, httptools, hf-xet, gguf, dnspython, diskcache, astor, airportsdata, watchfiles, tiktoken, starlette, opentelemetry-proto, opentelemetry-api, email-validator, depyf, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, lm-format-enforcer, fastapi, outlines_core, opentelemetry-sdk, mistral_common, fastapi-cli, xgrammar, outlines, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, compressed-tensors, opentelemetry-exporter-otlp\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.32.1\n",
            "    Uninstalling opentelemetry-api-1.32.1:\n",
            "      Successfully uninstalled opentelemetry-api-1.32.1\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.53b1\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.53b1:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.53b1\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.32.1\n",
            "    Uninstalling opentelemetry-sdk-1.32.1:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.32.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.3.19 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.3.17 requires tyro, which is not installed.\n",
            "vllm 0.8.4 requires ray[cgraph]!=2.44.*,>=2.43.0, which is not installed.\n",
            "unsloth 2025.3.19 requires protobuf<4.0.0, but you have protobuf 4.25.7 which is incompatible.\n",
            "unsloth 2025.3.19 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.18.0.dev0 which is incompatible.\n",
            "unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 4.25.7 which is incompatible.\n",
            "unsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.18.0.dev0 which is incompatible.\n",
            "vllm 0.8.4 requires numba==0.61.2; python_version > \"3.9\", but you have numba 0.60.0 which is incompatible.\n",
            "vllm 0.8.4 requires transformers>=4.51.1, but you have transformers 4.50.0.dev0 which is incompatible.\n",
            "vllm 0.8.4 requires xformers==0.0.29.post2; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have xformers 0.0.29.post3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "google-cloud-pubsub 2.29.0 requires opentelemetry-api>=1.27.0; python_version >= \"3.8\", but you have opentelemetry-api 1.26.0 which is incompatible.\n",
            "google-cloud-pubsub 2.29.0 requires opentelemetry-sdk>=1.27.0; python_version >= \"3.8\", but you have opentelemetry-sdk 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 blake3-1.0.4 compressed-tensors-0.9.3 depyf-0.18.0 diskcache-5.6.3 dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.12 fastapi-cli-0.0.7 gguf-0.16.2 hf-xet-1.0.4 httptools-0.6.4 importlib_metadata-8.0.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.19 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgspec-0.19.0 ninja-1.11.1.4 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.3 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 protobuf-4.25.7 pycountry-24.6.1 python-dotenv-1.1.0 python-json-logger-3.3.0 python-multipart-0.0.20 pyzmq-26.4.0 rich-toolkit-0.14.3 starlette-0.46.2 tiktoken-0.9.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 xgrammar-0.1.18\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "deb4f711d75c421a8a1f5437a796a67f",
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --no-deps unsloth vllm\n",
        "# [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "# Skip restarting message in Colab\n",
        "import sys, re, requests; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "!pip install git+https://github.com/huggingface/trl@main\n",
        "!pip install --no-deps bitsandbytes \"accelerate>=0.28\" xformers==0.0.29.post3 peft triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "# vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "    file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "!pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjKIMLVFENeI",
        "outputId": "1c9389dc-3935-49c9-a8b8-bd4fb81c1a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "c02698d024f5461bb508bf7795fdc83d",
            "474b883472bd438f80afb6f8f9bb9db0",
            "1b21948f70f84a9ca3a147c66c541379",
            "da2c6631722b45cc8bde8cdcda85c569",
            "978d1128d8a04343ad13af32d12f097c",
            "9a59159e7a104f5e91d4c97937382dbf",
            "9cc10974a45949a9bb974a4c70ceeb62",
            "6bb573e75a5a46df976d3281b9ab708e",
            "634b8a6242e047569015b6d4bd037e88",
            "76226fd3aee34b37a40d5b80df9c0542",
            "3a9b517119144927bfa10fa3da4b38a3",
            "afd764d7fefe455cb7d9d63e1cc74a99",
            "fd2b378dfc1842c4863a7b4e1c0b3b6a",
            "3a180c9cc1844f6b89dce9e63d042915",
            "0d6f791d9d5840a7ab247c1e416adb21",
            "e2a3eb0fb92b48c89219bf788d0d6fe9",
            "000c50171bd64d72903fcfc235666476",
            "e8517cbc74f34b3a94e3e7b542436c32",
            "ef1add11fb1a4e00beca34bc04cc099d",
            "8e72256682d04056bc216cc866a45c96"
          ]
        },
        "id": "zva0n0AuGL3M",
        "outputId": "bd1cf711-ba47-4966-eed0-49196831e8f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c02698d024f5461bb508bf7795fdc83d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# \n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "8ThXjpEAGQ99",
        "outputId": "897ee6a5-b9ad-4c2c-ef4c-8fbdd3d4d62c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-25 01:38:36 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-94aa0f5680f3>:15: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastModel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33max2119\u001b[0m (\u001b[33mmyprojs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/rl_project/baseline_checkpooints_gemma/wandb_logs/wandb/run-20250425_013843-l5pzv35h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/myprojs/rl_course_proj/runs/l5pzv35h' target=\"_blank\">dry-sky-42</a></strong> to <a href='https://wandb.ai/myprojs/rl_course_proj' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/myprojs/rl_course_proj' target=\"_blank\">https://wandb.ai/myprojs/rl_course_proj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/myprojs/rl_course_proj/runs/l5pzv35h' target=\"_blank\">https://wandb.ai/myprojs/rl_course_proj/runs/l5pzv35h</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import re\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import time\n",
        "# from huggingface_hub import login\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from transformers import logging as hf_logging\n",
        "import json\n",
        "from dataclasses import asdict, is_dataclass\n",
        "from unsloth import FastModel\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "\n",
        "# # Huggingface Login\n",
        "# login(token=\"\")\n",
        "\n",
        "# Wandb Login # \n",
        "wandb.login(key='YOUR_WANDB_KEY')\n",
        "run = wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"rl_course_proj\",\n",
        "    dir = \"/content/drive/MyDrive/rl_project/baseline_checkpooints_gemma/wandb_logs\"\n",
        ")\n",
        "\n",
        "# Huggingface cache dir\n",
        "hf_cache_dir = \"/content/drive/MyDrive/rl_project/hf_cache\"\n",
        "unsloth_cache_dir = \"/content/drive/MyDrive/rl_project/unsloth_cache\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "fe157cb587e041cc8e8f5f07b7a76936",
            "b87199fc633f49febccbac9fbe9c8429",
            "0040d9da53f142c997494fd201d10f3c",
            "28c1589c4e6043fd86d4658451e4daeb",
            "03af1481ba0f4f24a425c248444cc111",
            "d07382b5e84d42a8a23ad99a226635ff",
            "6162696dd7414b67a929955058c8807e",
            "2f1212d99f5a4bb7b515488642801c78",
            "641e23c914604b36a4c569e9db835846",
            "782a50cbddf4431086856736dd7e06e1",
            "fa899221c1724c24aa1475954617f6db",
            "f38a5519367e48c2bfcb4347b4634a43",
            "4a1932a9834045e1ace28edd57cc5de7",
            "47e6f54fe03c4975a85f46eb0149e30a",
            "730397835bf045928b7fe743ea860fb7",
            "86a80e5e8e2c4f39ab28e863ef19cf94",
            "4e02054ca80b41e4b30a512c3aee4981",
            "d81dab9de6b8483c9ea3843b77f51a0a",
            "1bc11bce418f4373af58ad489c6f172b",
            "02786284345a4260a9e8c479d83c2034",
            "bf46acb645c14d1a80cd492a2b2db829",
            "3eabef59e4174f3bb0ee5c1fd1ac644e"
          ]
        },
        "id": "rDgbqMfCEr3C",
        "outputId": "99a2a6c6-af5d-4079-a156-f46a485e4740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe157cb587e041cc8e8f5f07b7a76936",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f38a5519367e48c2bfcb4347b4634a43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n",
            "trainable params: 59,604,992 || all params: 4,359,684,464 || trainable%: 1.3672\n",
            "\n",
            "unsloth/gemma-3-4b-it Trainabale Parameters -->  None\n"
          ]
        }
      ],
      "source": [
        "#####################################################\n",
        "# LOADING THE MODEL\n",
        "max_seq_length = 1024\n",
        "lora_rank = 32\n",
        "model_id = \"unsloth/gemma-3-4b-it\"\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = model_id,\n",
        "    max_seq_length = max_seq_length, # We can choose longer for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False,\n",
        "    cache_dir = unsloth_cache_dir,\n",
        ")\n",
        "\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always\n",
        "\n",
        "    r = lora_rank,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = lora_rank,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(f\"\\n{model_id} Trainabale Parameters --> \",model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OtZC-PlD0fV",
        "outputId": "499b659b-2973-491b-a85c-1bc1d1f545b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Structure of the dataset:  Dataset({\n",
            "    features: ['question', 'answer'],\n",
            "    num_rows: 7473\n",
            "})\n",
            "\n",
            "One sample (first row) from the dataset:  {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n",
            "\n",
            "After mapping the first row {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': '72', 'prompt': [{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\nfirst thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning\\nprocess and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e.,\\n<think> reasoning process here </think> <answer> answer here </answer>.\\nThere should be only one instance of <think> </think> and <answer> </answer> tags, and not more in the correct output.', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]}\n"
          ]
        }
      ],
      "source": [
        "#####################################################\n",
        "# LOAD DATASET\n",
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
        "\n",
        "#####################################################\n",
        "# STRUCTURING THE DATASET\n",
        "print(\"\\nStructure of the dataset: \",dataset)\n",
        "\n",
        "# One sample from the dataset\n",
        "print(\"\\nOne sample (first row) from the dataset: \",dataset[0])\n",
        "\n",
        "# We notice all answers like about have a ####, so we extract it:\n",
        "def extract_hash_answer(text):\n",
        "    if \"####\" not in text: return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# We now create a system prompt which can be customized. We add 4 extra symbols for working out or thinking / reasoning sections and a final answer:\n",
        "reasoning_start = \"<think>\"\n",
        "reasoning_end   = \"</think>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\n",
        "first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning\n",
        "process and answer are enclosed within {reasoning_start} {reasoning_end} and {solution_start} {solution_end} tags, respectively, i.e.,\n",
        "{reasoning_start} reasoning process here {reasoning_end} {solution_start} answer here {solution_end}.\n",
        "There should be only one instance of {reasoning_start} {reasoning_end} and {solution_start} {solution_end} tags, and not more in the correct output.\"\"\"\n",
        "\n",
        "# Let's map the dataset! and see the first row:\n",
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
        "})\n",
        "print(\"\\nAfter mapping the first row\",dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pceVaUfvBdzj"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "# SETTING THE REWARD FUNCTIONS\n",
        "\n",
        "# We create a regex format to match the reasoning sections and answers:\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\\\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "\n",
        "# Format Reward\n",
        "# We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:\n",
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 1.0\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "# Accuracy Reward\n",
        "# Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios\n",
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    # print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Correct answer gets 3 points!\n",
        "        if guess == true_answer:\n",
        "            score += 1.0\n",
        "        # Match if spaces are seen\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 0.5\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "# Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
        "number_pat = (\n",
        "    r\"[-+]?\"                      # optional sign\n",
        "    r\"(?:\\d+\\.\\d*|\\.\\d+|\\d+)\"     # int or float (requires a digit somewhere)\n",
        "    r\"(?:[eE][-+]?\\d+)?\"          # optional exponent\n",
        ")\n",
        "\n",
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?({number_pat})\",\n",
        "    flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            guess       = float(guess.strip())\n",
        "            scores.append(0.5 if guess == true_answer else 0.0)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBnkWc9EHTnI",
        "outputId": "964f238b-23ed-4bbb-9145-7f62bdbe7312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into '/content/aarjun0112/baseline_checkpoint_gemma_simple'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 50 (delta 10), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (50/50), 55.50 KiB | 2.31 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "hub_repo   = \"aarjun0112/baseline_checkpoint_gemma_simple\"           # create it once on the HF website\n",
        "hf_token   = \"YOUR_HF_KEY\"   # or paste the token string here\n",
        "\n",
        "run_stamp  = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "output_dir = Path(\"/content\") / hub_repo       # ⭐ clone repo locally as the output dir\n",
        "!git lfs install --skip-smudge                 # avoids pulling large files on clone\n",
        "!git clone https://huggingface.co/{hub_repo} {output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJKy4W_eExgc",
        "outputId": "e3252910-3ff8-4704-dd79-d68f4b433a54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "#####################################################\n",
        "# SETTING THE GRPO TRAINING HYPER-PARAMETERS\n",
        "max_prompt_length = 384\n",
        "max_completion_length = max_seq_length - max_prompt_length # 768 if max_seq_length - 1024\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=str(output_dir),\n",
        "    run_name = run_stamp,\n",
        "\n",
        "    learning_rate = 5e-6,\n",
        "    # adam_beta1 = 0.9,\n",
        "    # adam_beta2 = 0.99,\n",
        "    # weight_decay = 0.1,\n",
        "    # warmup_ratio = 0.1,\n",
        "    warmup_steps = 15,\n",
        "    # lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    max_grad_norm = 0.2,\n",
        "\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "\n",
        "    push_to_hub     = True,                    # ← magic switch to save in hf hub\n",
        "    hub_token       = hf_token,\n",
        "    hub_model_id    = hub_repo,\n",
        "\n",
        "    # loss_type = \"dr_grpo\",\n",
        "    mask_truncated_completions = True,\n",
        "    # scale_rewards=False,\n",
        "    beta= 0.001,\n",
        "    # num_iterations=2,\n",
        "\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 6, # Increase for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "\n",
        "    num_train_epochs = 5, # Set to 1 for a full training run\n",
        "\n",
        "    report_to=\"wandb\",\n",
        "    logging_strategy=\"steps\",         # more explicit\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,          # log step‑0 metrics\n",
        "    log_level=\"info\",                 # HF logger verbosity\n",
        "    disable_tqdm=False,               # keep the progress bar\n",
        "\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=10,\n",
        "    save_total_limit=5,\n",
        ")\n",
        "\n",
        "# writes training_args.json\n",
        "cfg_path = Path(\"/content/training_args.json\")\n",
        "\n",
        "# Works for both dataclass- and plain‑object configs\n",
        "cfg_dict = asdict(training_args) if is_dataclass(training_args) else vars(training_args)\n",
        "\n",
        "with cfg_path.open(\"w\") as f:\n",
        "    json.dump(cfg_dict, f, indent=2)\n",
        "\n",
        "#####################################################\n",
        "# TRAINING THE MODEL\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        # match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "\n",
        "hf_logging.set_verbosity_info()                 # shows every call & warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wCHH1T4hGj4U",
        "outputId": "237f55f7-fa30-45aa-80c3-7ddcc084ede2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 7,473 | Num Epochs = 5 | Total steps = 37,365\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 6\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 6 x 1) = 6\n",
            " \"-____-\"     Trainable parameters = 59,604,992/4,359,684,464 (1.37% trained)\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************** Question:\n",
            "Ahmed and Emily are having a contest to see who can get the best grade in the class. There have been 9 assignments and Ahmed has a 91 in the class. Emily has a 92. The final assignment is worth the same amount as all the other assignments. Emily got a 90 on the final assignment. What is the minimum grade Ahmed needs to get to beat Emily if all grades are whole numbers? \n",
            "Answer:\n",
            "100 \n",
            "Response:\n",
            "Here’s the solution breakdown:\n",
            "\n",
            "<think>Ahmed needs to achieve a minimum score to surpass Emily's grade. Emily's score stands at 92, and Ahmed's current score is 91. Considering the final assignment holds the same weight as all others, and Emily achieved 90 on the final, Ahmed must achieve a score greater than 92 (to be able to beat Emily’s grade) with whole numbers. </think> <answer>Ahmed needs a minimum score of 93 to beat Emily's grade. </answer> \n",
            "Extracted:\n",
            "93\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='37365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   30/37365 24:58 < 554:59:29, 0.02 it/s, Epoch 0.00/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>-0.037800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>-0.170500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************** Question:\n",
            "The gauge on a water tank shows that the tank is 1/3 full of water. To fill the tank, 16 gallons of water are added. How many gallons of water does the tank hold when full? \n",
            "Answer:\n",
            "24 \n",
            "Response:\n",
            "Here's the solution, incorporating the requested elements:\n",
            "\n",
            "<think>The tank is 1/3 full, and 16 gallons are added to fill it.  Therefore, 16 gallons represent 2/3s of the tank'ss capacity.  Let 'x' be the full capacity.  So, (2/3)x = 16.  Multiplying by 3/2, we get x = 16 * (3/2) = 24 gallons. </think> <answer>The tank holds 24 gallons when full. </answer> \n",
            "Extracted:\n",
            "24\n",
            "******************** Question:\n",
            "There are 15 tables in the school's cafeteria. Each table can seat 10 people. Usually, only 1/10 of the seats are left unseated. How many seats are usually taken? \n",
            "Answer:\n",
            "135 \n",
            "Response:\n",
            "Okay, let’s solve this problem step-by-step, keeping in mind the structure of the solution.\n",
            "\n",
            "<think>\n",
            "First, let's determine the total number of seats in the cafeteria. There are 15 tables, and each table can seat 10 people, so the total number of seats is 15 * 10 = 150 seats.\n",
            "\n",
            "Next, let’s calculate the number of seats usually left unseated. Based on the given information, usually, 1/10 of the seats are left unseated. Therefore, the number of seats unseated is (1/10) * 150 = 15 seats.\n",
            "\n",
            "Finally, to determine the number of seats usually taken, subtract the number of unseated seats from the total number of seats: 150 - 15 = 135 seats.\n",
            "</think>\n",
            "\n",
            "<answer>\n",
            "135 seats are usually taken.\n",
            "</answer> \n",
            "Extracted:\n",
            "135\n",
            "******************** Question:\n",
            "Fiona completed 36 math questions in an hour. Shirley was able to complete twice as many math questions within that same time, and Kiana completed half of the sum of Fiona and Shirley's math questions. If they each did the same number of questions the following hour, how many math questions did all three girls complete in 2 hours? \n",
            "Answer:\n",
            "324 \n",
            "Response:\n",
            "Here's a solution addressing the prompt's requirements:\n",
            "\n",
            "<think>Fiona completed 36 math questions in an hour. Shirley was able to complete twice as many math questions within that same time, and Kiana completed half of the sum of Fiona and Shirley's math questions. If they each did the same number of questions the following hour, how many math questions did all three girls complete in 2 hours?\n",
            "\n",
            "1. **Calculate Shirley's completed questions:** Shirley completed twice as many as Fiona (36), meaning she completed 2 * 36 = 72 questions in an hour.\n",
            "\n",
            "2. **Calculate Kiana's completed questions:** Kiana completed half of Fiona *and* Shirley's combined questions: Half of (36 + 72) = Half of 108 = 54 questions.\n",
            "\n",
            "3. **Calculate total questions completed in the first hour:** Fiona + Shirley + Kiana = 36 + 72 + 54 = 162 questions.\n",
            "\n",
            "4. **Calculate total questions completed in the second hour:** Since they each did the same number of questions as in the first hour (162), the total for the second hour is 3 * 162= 162 questions.\n",
            "\n",
            "5. **Calculate total questions completed in 2 hours:** 162 + 162 = 324 questions.\n",
            "\n",
            "</think> <answer>324</answer> \n",
            "Extracted:\n",
            "324\n",
            "******************** Question:\n",
            "In a graveyard, there are 20 skeletons.  Half of these skeletons are adult women, and the remaining number are split evenly between adult men and children.  If an adult woman has 20 bones in their body, and a male has 5 more than this, and a child has half as many as an adult woman, how many bones are in the graveyard? \n",
            "Answer:\n",
            "375 \n",
            "Response:\n",
            "Here's the breakdown of the solution, following the requested format:\n",
            "\n",
            "<think>\n",
            "1. **Calculate adult women:** Half of 20 skeletons are adult women: 20 / 2 = 10 skeletons.\n",
            "2. **Calculate adult men:** 10 adult women *are* split evenly between adult men and children so: 10 skeletons.\n",
            "3. **Calculate children:** 10 skeletons are split evenly between adult men and children, so: 10 / 2 = 5 skeletons.\n",
            "4. **Calculate total bones** 20 skeletons + 10 adult men + 5 children = 35 bones.\n",
            "5. **Calculate total bones** 20 skeletons + 10 adult women + 5 children = 35 bones.\n",
            "6. **Calculate total bones** 20 skeletons + 10 adult women + 5 children = 35 bones.\n",
            "7. **Consider adult woman bones** 10 adult women * 20 bones each = 100 bones.\n",
            "8. **Calculate male bones** 10 adult men * 25 bones each = 250 bones.\n",
            "9. **Calculate child bones** 5 children * 10 bones each == 50.\n",
            "10. **Total bones** 100 + 250 + 50 = 400.\n",
            "</think> <answer>400</answer> \n",
            "Extracted:\n",
            "400\n",
            "******************** Question:\n",
            "Lynne bought 7 books about cats and 2 books about the solar system. She also bought 3 magazines. Each book cost 7$ and each magazine cost $4. How much did Lynne spend in all? \n",
            "Answer:\n",
            "75 \n",
            "Response:\n",
            "Okay, here’s the solution to the problem, broken down with the requested tags:\n",
            "\n",
            "<think>Lynne spent 7 books * 7$ + 2 books * 7$ + 3 magazines * 4$ = 49$ + 14$ + 12$ = 75$.</think> <answer>Lynne spent $75 in all.</answer> \n",
            "Extracted:\n",
            "75\n",
            "******************** Question:\n",
            "Every Monday to Friday morning, Alice walks 10 miles through a large grass field to get to school. To avoid the hot afternoon sun on her way home, she walks 12 miles through a forest. When she gets home on Friday, how many miles will she have walked that week? \n",
            "Answer:\n",
            "110 \n",
            "Response:\n",
            "Okay, let’s work through this problem.\n",
            "\n",
            "<think>Alice walks 10 miles to school and 12 miles through the forest. To find the total, we add these two distances together. 10 + 12 = 22. So, she has walked 22 miles that week.</think> <answer>22 miles</answer> \n",
            "Extracted:\n",
            "22\n",
            "******************** Question:\n",
            "Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity? \n",
            "Answer:\n",
            "108 \n",
            "Response:\n",
            "Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?\n",
            "\n",
            "<think>Sam's popsicle sticks = 3 * 12 = 36. Sid's popsicle sticks = 2 * 12 = 24. Sam's popsicle sticks total = 36 + 24 = 60. </think> <answer>60</answer> \n",
            "Extracted:\n",
            "60\n",
            "******************** Question:\n",
            "Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages? \n",
            "Answer:\n",
            "7 \n",
            "Response:\n",
            "Here’s the solution to Marie’s problem:\n",
            "\n",
            "<think>Marie starts with 98 unread messages. She reads 20 messages a day but also receives 6 new messages a day. This means her unread message count increases by (6 - 20) =14 messages a day. To read all messages, she will need to reduce her count by 98. The number of days it will take to do is 98/14=7</think> <answer>It will take her 7 days to read all her unread messages.</answer> \n",
            "Extracted:\n",
            "7\n",
            "******************** Question:\n",
            "Mel is three years younger than Katherine.  When Katherine is two dozen years old, how old will Mel be in years? \n",
            "Answer:\n",
            "21 \n",
            "Response:\n",
            "Okay, let's break down this problem and solution.\n",
            "\n",
            "<think>Mel is three years younger than Katherine. This means Katherine is always six years older than Mel.\n",
            "When Katherine is two dozen (24) years old, Mel will be 24 - 6 = 18 years old. </think> <answer>Mel will be 18 years old. </answer> \n",
            "Extracted:\n",
            "18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-10\n",
            "Image processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-10/preprocessor_config.json\n",
            "chat template saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-10/chat_template.json\n",
            "processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-10/processor_config.json\n",
            "Image processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/preprocessor_config.json\n",
            "chat template saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/chat_template.json\n",
            "processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/processor_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************** Question:\n",
            "There are 84 people waiting in line to ride a roller coaster at an amusement park.  The roller coaster has 7 cars, and each car seats 2 people.  How many times will the ride operator have to run the roller coaster to give everyone in line a turn? \n",
            "Answer:\n",
            "6 \n",
            "Response:\n",
            "Okay, here’s the solution and breakdown, following the requested format and constraints:\n",
            "\n",
            "<think>The reasoning process is this: We need to figure out how many 'rides' are required to give everyone a turn. This means 'riding through' the total number of people in line. Each ride can accommodates 2 people. To find out how many 'rides' are needed, we divide the total number of people in the line by the total number of people each 'ride’ can accommodate: '84 / 2 = ‘42’ . Therefore, 42 ‘rides’ are needed. </think> <answer>42</answer> \n",
            "Extracted:\n",
            "42\n",
            "******************** Question:\n",
            "Layla is feeding her pet fish. She has two Goldfish which each get one teaspoon of fish food. Her 3 Swordtails each get 2 teaspoons of food. Her 8 Guppies each eat half a teaspoon of food. How much food does she have to give to her fish in total? \n",
            "Answer:\n",
            "12 \n",
            "Response:\n",
            "Okay, here’s a breakdown of the solution, following the specified format:\n",
            "\n",
            "<think>Laylaa’ssefeeddinggerepetfish.ShehastwoGoldfishwhichheeaggetonespoonoffishfood.Her3Swordtailsachetg2teaspoonsoffood.Her8Guppieseachgreatalfateaspoonoffood.Howmuchfooddoeshesesetogivetohisfinhtalt?\n",
            "\n",
            "Let's calculate the total amount of food:a\n",
            "\n",
            "*   **Goldfish:**a 2 Goldfish * 1 teaspoon/Goldfish = 2 teaspoonsa\n",
            "*   **Swordtails:**a 3 Swordtails * 2 teaspoons/Swordtail = 6 teaspoonsa\n",
            "*   **Guppies:**a 8 Guppies * 0.5 teaspoon/Guppy = 4 teaspoonsa\n",
            "\n",
            "**Total Food:**a 2 teaspoons + 6 teaspoons + 4 teaspoons = 12 teaspoonsa\n",
            "\n",
            "</think> <answer>Laylaahastentogivetoherfishaltintotal. </answer>a\n",
            "\n",
            "**Answer:**a Laylaahastentogivetoherfishaltintotalof12teaspoonsa.a\n",
            "\n",
            "a</answer>a\n",
            "\n",
            "a</answer>a\n",
            "\n",
            "a</answer>a\n",
            "\n",
            "a)a\n",
            "\n",
            "a\n",
            "\n",
            "aaaaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa\n",
            "\n",
            "aaaa \n",
            "Extracted:\n",
            "12\n",
            "******************** Question:\n",
            "Nina makes one-of-a-kind jewelry and sells it at a local boutique.  She charges $25.00 for her necklaces, $15.00 for bracelets and $10.00 for a pair of earrings.  Over the weekend, she sold 5 necklaces, 10 bracelets, 20 earrings, and received 2 orders for a complete jewelry ensemble that Nina charged $45.00 for.  How much money did she make over the weekend? \n",
            "Answer:\n",
            "565 \n",
            "Response:\n",
            "Okay, let’s break down Nina’s weekend earnings.\n",
            "\n",
            "<think> Nina’s revenue comes from selling individual items (necklaces, bracelets, earrings) at a fixed price and from also from an order of a few-items-ensemble.\n",
            "*   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   *   * \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "The Early Bird Dinner offered dinner meals at half off the menu price if you eat between 2-4 pm.  Curtis ordered the Salisbury Steak that costs $16.00 and Rob ordered the Chicken Fried Steak at $18.00.  If they ate at 3 pm, what was the cost of their total bill? \n",
            "Answer:\n",
            "17 \n",
            "Response:\n",
            "Here’s the solution to the problem, incorporating the requested elements:\n",
            "\n",
            "<think>Curtis’s Salisbury Steak cost $16.00 and Rob’s Chicken Fried Steak cost $18.00.  They ate at 3 pm, which falls within the offered-half-price-time frame of 2-4 pm. Therefore, the cost of their total bill will be half of the total cost of their orders before any discounts.  Total cost before discount = $16.00 + $18.00 = $34.00.  Half of this total cost = $34.00 / 2 = $17.00</think> <answer>The total cost of their bill was $17.00.</answer> \n",
            "Extracted:\n",
            "17.00\n",
            "******************** Question:\n",
            "Boston had .5 feet of snow on the first day of winter.  The next day they got an additional 8 inches.  Over the next 2 days, 2 inches of the snow melted.  On the fifth day, they received another 2 times the amount of snow they received on the first day.  How many feet of snow do they now have? \n",
            "Answer:\n",
            "2 \n",
            "Response:\n",
            "Here’s an analysis of the problem and the solution, enclosed within the requested tags:\n",
            "\n",
            "<think>\n",
            "The problem describes a series of events related to the accumulation and melting of snow in Boston. The key is to first identify the amounts of snow received on each day and then to add or subtract them as necessary, keeping the final answer in feet.\n",
            "</think>\n",
            "\n",
            "<answer>\n",
            "<think>\n",
            "Day 1: 0.5 feet\n",
            "Day 2: 8 inches = 8/12 = .6667 feet\n",
            "Day 3: 2 inches = .16667 feet\n",
            "Day 4: 2 inches = .16667 feet\n",
            "Day 5: 2 * 0.5 = 1 foot\n",
            "Total Snow = 0.5 + 0.6667 + 0.1667 + 0.1667 + 1 = 2.5667 feet\n",
            "</think>\n",
            "<answer>\n",
            "2.5667\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</answer>\n",
            "\n",
            "</ \n",
            "Extracted:\n",
            "1\n",
            "******************** Question:\n",
            "At Hank’s cafe, he sells big stack pancakes which have 5 pancakes and short stack pancakes which have 3 pancakes.  If 9 customers order the short stack and 6 customers order the big stack, how many pancakes does Hank need to make? \n",
            "Answer:\n",
            "57 \n",
            "Response:\n",
            "Here’s an solution to the problem, incorporating the requested elements:\n",
            "\n",
            "<think>Okay, let’s first break down the number of pancakes needed for each order.  Short stack is ordered by 9 customers, and since each short stack contains 3 pancakes, the total number of pancakes for short stack orders is 9 * 3 = 27.  Big stack is ordered by 6 customers, and since each big stack contains 5 pancakes, the total number of pancakes for big stack orders is 6 * 5 = 30.  Now, let’s just sum up the total number of pancakes needed for both short stack and big stack orders : 27 + 30 = 57. </think> <answer>Hank needs to make 57 pancakes</answer> \n",
            "Extracted:\n",
            "57\n",
            "******************** Question:\n",
            "Mary bought a packet of 1500 stickers. She shared them between Susan, Andrew and Sam in the ratio 1:1:3 respectively. If Sam gave Andrew two-thirds of his own share, how many stickers does Andrew now have? \n",
            "Answer:\n",
            "900 \n",
            "Response:\n",
            "Here's the solution to the problem, incorporating the requested elements:\n",
            "\n",
            "<think>Mary’s stickers: 1500 stickers. Ratio: 1:1:3. Sam’s share ratio: 3. So Sam’s stickers: 1500 * 3 / 13 = 1152. Sam gives Andrew 2/3 of his share. Andrew’s share: 1152 * 2/3 = 768. </think> <answer>Andrew now has 768 stickers. </answer> \n",
            "Extracted:\n",
            "768\n",
            "******************** Question:\n",
            "Milo can roll downhill on his skateboard at twice the speed that he can run.  And Milo's best friend, Cory, can drive his wheelchair at twice the speed that Milo can roll downhill on his skateboard.  If Cory always drives his wheelchair at 12 miles per hour, how many miles can Milo run in two hours? \n",
            "Answer:\n",
            "6 \n",
            "Response:\n",
            "Here's the solution to the problem:\n",
            "\n",
            "<think>Cory's best friend's wheelchair’s speed = 12 miles per hour.\n",
            "Cory’s best friend’s wheelchair’s speed / 2 = Milo’s best friend’s wheelchair’s speed.\n",
            "Milo’s best friend’s wheelchair’s speed / 2 / 2 = Milo’s best friend’s wheelchair’s speed/ 4 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s speed / 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s speed / 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s best friend’s wheelchair’s speed = 12 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed/ 2 = Milo’s best friend’s best friend’s speed.\n",
            "Milo’s best friend’s best friend’s speed = 6 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed/ 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s best friend’s speed = 3 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed = 3 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed/ 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s best friend’s speed = 6 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed / 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s best friend’s speed = 3 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed = 3 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed / 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s best friend’s speed = 6 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed = 3 miles per hour.\n",
            "Milo’s best friend’s speed = 1.5 miles per hour.\n",
            "\n",
            "Milo’s best friend’s speed = 1.5 miles per hour.\n",
            "Milo’s best friend’s speed / 2 = Milo’s best friend’s speed.\n",
            "Milo’s best friend’s speed = 0.75 miles per hour.\n",
            "Milo’s best friend’s speed = 0.75 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed = 1.25 miles per hour.\n",
            "Milo’s best friend’s best friend’s speed / 2 = Milo’s best friend’s best friend’s speed.\n",
            "M \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "Connie is trying to remember when her grandmother was born. She knows her grandmother's older brother was born in 1932, her older sister was born in 1936, and the gap between her grandmother and her sister is twice the gap between the older brother and the older sister. What year was Connie's grandma born? \n",
            "Answer:\n",
            "1944 \n",
            "Response:\n",
            "Here’s an solution to Connie’s question:\n",
            "\n",
            "<think>Theodore was born in 1932. Beatrice was born in 1936. The difference between Theodore and Beatrice is 4 years. Connie’s grandmother is twice that difference. So, the grandmother was born 8 years before Theodore. That makes the grandmother’s year of birth 1932 - 8 = 1924. Finally, Connie’s grandmother’s older sister Beatrice was born in 1936. That means Connie’s grandmother’s older sister Beatrice was born 8 years after 1924, making the year 1936. Connie’s sister Beatrice was born in 1936. Connie’s grandmother was born in 1924. Her grandmother’s older brother Theodore was born in 1932. Her older sister Beatrice was born in 1936. The gap between Theodore and Beatrice is 4. The gap between the older brother and the older sister is 4. Her grandmother’s older brother and sister’s gap is 4. Her grandmother’s older brother and sister’s gap is also 4. Her grandmother’s older brother and sister’s gap is 4. Her grandmother’s older brother and sister’s gap is 4. So, her grandmother’s older brother and sister’s gap is 4. However, the gap between her grandmother and her sister is twice the gap between the older brother and the older sister. That means her grandmother’s older brother and sister’s gap is 4. The difference is 4. Twice that difference is 8. So, it means her grandmother’s older sister was born in 1936. Therefore, her grandmother’s year of birth is 1924.</think> <answer>1924</answer> \n",
            "Extracted:\n",
            "1924\n",
            "******************** Question:\n",
            "The price per organic egg is 50 cents. The price of a tray of eggs that contains 30 eggs is $12. How much can you save, in cents, per egg if you buy a tray versus individual eggs? \n",
            "Answer:\n",
            "10 \n",
            "Response:\n",
            "Here's the breakdown of the solution, following the constraints you've provided:\n",
            "\n",
            "<think>  Let's calculate the cost savings per egg.\n",
            "\n",
            "1. **Cost of tray:** $12 / 30 eggs = $0.40 per egg.\n",
            "2. **Cost of individual eggs:** $0.50 per egg.\n",
            "3. **Cost difference:** $0.50 - $0.40 = $0.10 per egg.\n",
            "4. **Cost savings per egg:** $0.10 per egg.\n",
            "\n",
            "</think> <answer> $0.10 per egg.  Since we need to answer in cents, the answer is 10 cents. </answer> \n",
            "Extracted:\n",
            "0.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-20\n",
            "Image processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-20/preprocessor_config.json\n",
            "chat template saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-20/chat_template.json\n",
            "processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/checkpoint-20/processor_config.json\n",
            "Image processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/preprocessor_config.json\n",
            "chat template saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/chat_template.json\n",
            "processor saved in /content/aarjun0112/baseline_checkpoint_gemma_simple/processor_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************** Question:\n",
            "Sebastian plays drums for percussion in an orchestra. He sits next to the brass section where four people play trombone, two play trumpet, and one plays a French horn. In front of them are the strings section with three violinists, a cellist, and a contrabassist. Next to the strings are a trio of clarinets and a quartet of flute players in the woodwinds section. The maestro conducts them all. How many people are in the orchestra? \n",
            "Answer:\n",
            "21 \n",
            "Response:\n",
            "Here's an analysis of the problem and a solution, adhering to the constraints:\n",
            "\n",
            "<think>The problem asks for the number of people in the orchestra.  It’s described as having sections: strings, woodwinds, and brass. We must count every person described in each section.  The strings section has three violinists, a cellist, and a contrabassist (5 people). The woodwinds section has three violinists, a cellist, and a contrabassist (5 people). The brass section has four trombone players, two trumpet players, and one French horn player (7 people). The maestro conducts them all.  Adding these up, 5+5+7+1 = 18 people</think> <answer>18</answer> \n",
            "Extracted:\n",
            "18\n",
            "******************** Question:\n",
            "Bobby buys two packets of candy. He eats two candies every day from Monday through Friday and takes one each during the remaining days of the week. If it takes him 3 such weeks to finish the 2 packets, how many candies are in a packet? \n",
            "Answer:\n",
            "18 \n",
            "Response:\n",
            "Here's a solution to the problem, incorporating the requested elements:\n",
            "\n",
            "Bobby buys two packets of candy. He eats two candies every day from Monday through Friday and takes one each during the remaining days of the week. If it takes him 3 such weeks to finish the 2 packets, how many candies are in a packet?\n",
            "\n",
            "<think>Let 'x' be the number of candies in a packet. Bobby consumes 2 packets * 3 weeks * 7 days/week * 2 candies/day = 84 candies. This is also equivalent to 2 packets * 3 weeks * 5 days/week * 1 candy/day + 2 packets * 3 weeks * 2 days/week * x/week. It’s also equivalent to 2 * 3 * 7 * 2 + 2 * 3 * 5 * x = 84. Hence 84 = 84 + 0x which make sense. Now from 2 packets * 3 weeks * 5 days/week * x/week = 84. Hence 30x = 84. Therefore x = 84/30 = 28/5/3 = 28/30 = 14/15. We should write 26/30 = 13/15. Now let’s verify: 3 packets * 7 days * 2 candies = 42. Then 3 * 7 * 2 = 42, and 3 * 7 = 21, so 21 * 2 = 42. The total candies is 42, which is 2 packets. Hence packets = 24/3.\n",
            "Now the total number of candies is 2 * 3 * 7 * 2 + 2 * 3 * 5 * x = 84. Hence 84 + 30x = 84. Hence 30x = 0. Therefore x = 0. That is not the case.\n",
            "\n",
            "Let packet = x, then candies = 2x. Over 3 weeks, he consumes 2 packets * 3 weeks * 7 days/week * 2 candies/day = 84 candies.\n",
            "Also, he consumes 2 packets * 3 weeks * 5 days/week * 1 candy/day = 30 candies.\n",
            "So, 2*3 * 7 * 2 = 84 candies, and 2 * 3 * 5 * x candies = 30.\n",
            "Therefore, 30x = 30, so x = 1.\n",
            "Total candies = 84 + 30 = 114.  Thus 2 packet of candy is 114.\n",
            "So, the number of candies in one packet is 114/2 = 57. </think>\n",
            "\n",
            "<answer>57</answer> \n",
            "Extracted:\n",
            "57\n",
            "******************** Question:\n",
            "For a school fundraiser, Tory needs to sell 50 packs of cookies. So far, he has sold 12 packs to his grandmother, 7 packs to his uncle, and 5 packs to a neighbor. How many more packs of cookies does Tory need to sell? \n",
            "Answer:\n",
            "26 \n",
            "Response:\n",
            "Okay, here's a solution to Tory's cookie sales problem, following the guidelines you've provided:\n",
            "\n",
            "<think>Toryy needs to sell 50 packs of cookies. He has sold 12 packs to his grandmother, 7 packs to his uncle, and 5 packs to a neighbor.  Therefore, he needs to sell 50 - 12 - 7 - 5 packs. 50 - 12 is 38. 38 - 7 is 31. 31 - 5 is 26. So Tory needs to sell 26 packs more.</think> <answer>Tory needs to sell 26 more packs of cookies.</answer> \n",
            "Extracted:\n",
            "26\n",
            "******************** Question:\n",
            "Mary only has 8 screws on hand and needs to buy 2 times more to fix her sink. When she is beginning repairs, she has to split the screws into four sections. How many are in each pile? \n",
            "Answer:\n",
            "6 \n",
            "Response:\n",
            "Okay, here’s a breakdown of Mary's situation, following the requested format:\n",
            "\n",
            "<think>Mary starts with 8 screws. She needs to buy 2 * 8 = 16 screws. Totaling 8 + 16 = 24 screws.  Sheals these 24 screws into 4 sections, meaning 24 / 4 = 6 screws per pile.</think> <answer>There are 6 screws in each pile.</answer> \n",
            "Extracted:\n",
            "6\n",
            "******************** Question:\n",
            "A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether? \n",
            "Answer:\n",
            "2 \n",
            "Response:\n",
            "Here’s an attempt to answer the traveler’s question, incorporating the rules and constraints:\n",
            "\n",
            "<think>The traveler drank 32 ounces. His camel drank 7 * 32 = 224 ounces. Total ounces = 32 + 224 = 256 ounces.  Since 128 ounces = 1 gallon, then 256 ounces = 2 gallons. </think><answer>They drank altogether 2 gallons of water.</answer> \n",
            "Extracted:\n",
            "2\n",
            "******************** Question:\n",
            "Three old cars displayed in an auction event have different manufacture dates. The first car, made in 1970,  was made 10 years earlier than the second car. The third car was manufactured 20 years later after the second car was manufactured. Calculate the year that the third car was made. \n",
            "Answer:\n",
            "2000 \n",
            "Response:\n",
            "Okay, here’s the solution, following your guidelines:\n",
            "\n",
            "<think>Let's break down this problem. First, we know the second car was made 10 years *after** 1970. So, that’s makes 1970 + 10 = 1980. Then, we’re told the third car was made 20 years *after** 1980. So, that’s makes 1980 + 20 = 2000. Therefore, the third car was manufactured in 2000.</think> <answer>2000</answer> \n",
            "Extracted:\n",
            "2000\n",
            "******************** Question:\n",
            "At Mario's barbershop haircuts are 50% more expensive during the weekends. If Mario paid $18 for his last haircut on Monday, how much he would have paid the day before? \n",
            "Answer:\n",
            "27 \n",
            "Response:\n",
            "Okay, let’s break down this problem step by step.\n",
            "\n",
            "<think>  The problem states that Mario’s last haircut cost $18, and that this was on Monday. We are asked to find out how much he would have paid the day before, which is Sunday.  Since the problem states that haircuts are 50% more expensive *during* the weekends, we can find out the price from Monday. The price is $18. However, it’s 50% more expensive than Monday during the weekend. So,  $18 * 1.50 = $27. Therefore, the price for Sunday is $27.</think> <answer>  $27 </answer> \n",
            "Extracted:\n",
            "27\n",
            "******************** Question:\n",
            "Janet has a business selling custom collars for dogs and cats. If it takes 18 inches of nylon to make a dog collar and 10 inches to make a cat collar, how much nylon does she need to make 9 dog collars and 3 cat collars? \n",
            "Answer:\n",
            "192 \n",
            "Response:\n",
            "Okay, let’s work through this problem.\n",
            "\n",
            "<think>Okay, Janet needs to make 9 dog collars, and each dog collar requires 18 inches of nylon. So that’s 9 * 18 = 162 inches of nylon for the dog collars. She also needs to make 3 cat collars, and each cat collar requires 10 inches of nylon. So that’s 3 * 10 = 30 inches of nylon for the cat collars. She needs a total of 162 + 30 = 192 inches of nylon. </think> <answer>Janet needs 192 inches of nylon to make 9 dog collars and 3 cat collars. </answer> \n",
            "Extracted:\n",
            "192\n",
            "******************** Question:\n",
            "Jonsey is awake for 2/3 of the day and spends 1/2 her time awake playing outside and the rest inside. Her brother, Riley, is awake for 3/4 of the day and spends 1/3 of his day outside and the rest inside. How much time, on average, do they spend inside? \n",
            "Answer:\n",
            "10 \n",
            "Response:\n",
            "Here's a solution to the problem, incorporating the constraints you've provided:\n",
            "\n",
            "<think>Jonsey is awake for 2/3 of the day, meaning 1/3 of the day is spent inside. She spends 1/2 her time awake outside, so 1/2 her time awake is spent inside. Therefore, she spends (2/3) * (1/2) = 1/3 of the day inside. Riley is awake for 3/4 of the day, meaning 1/4 of the day is spent inside. He spends 1/3 of his day outside, so 2/3 of his day is spent inside. Therefore, he spends (3/4) * (2/3) = 1/2 of the day inside. The average time they spend inside is (1/3) + (1/2) = 5/6 of the day inside. </think> <answer>5/6 of the day inside.</answer> \n",
            "Extracted:\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIK5pzQkI_AF"
      },
      "outputs": [],
      "source": [
        "!mount | grep drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQazIn9BvGiT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000c50171bd64d72903fcfc235666476": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0040d9da53f142c997494fd201d10f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f1212d99f5a4bb7b515488642801c78",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_641e23c914604b36a4c569e9db835846",
            "value": 2
          }
        },
        "02786284345a4260a9e8c479d83c2034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03af1481ba0f4f24a425c248444cc111": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d6f791d9d5840a7ab247c1e416adb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1b21948f70f84a9ca3a147c66c541379": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_76226fd3aee34b37a40d5b80df9c0542",
            "placeholder": "​",
            "style": "IPY_MODEL_3a9b517119144927bfa10fa3da4b38a3",
            "value": ""
          }
        },
        "1bc11bce418f4373af58ad489c6f172b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c1589c4e6043fd86d4658451e4daeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782a50cbddf4431086856736dd7e06e1",
            "placeholder": "​",
            "style": "IPY_MODEL_fa899221c1724c24aa1475954617f6db",
            "value": " 2/2 [00:07&lt;00:00,  3.68s/it]"
          }
        },
        "2f1212d99f5a4bb7b515488642801c78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a180c9cc1844f6b89dce9e63d042915": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9b517119144927bfa10fa3da4b38a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eabef59e4174f3bb0ee5c1fd1ac644e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "474b883472bd438f80afb6f8f9bb9db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb573e75a5a46df976d3281b9ab708e",
            "placeholder": "​",
            "style": "IPY_MODEL_634b8a6242e047569015b6d4bd037e88",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "47e6f54fe03c4975a85f46eb0149e30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc11bce418f4373af58ad489c6f172b",
            "max": 1155396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02786284345a4260a9e8c479d83c2034",
            "value": 1155396
          }
        },
        "4a1932a9834045e1ace28edd57cc5de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e02054ca80b41e4b30a512c3aee4981",
            "placeholder": "​",
            "style": "IPY_MODEL_d81dab9de6b8483c9ea3843b77f51a0a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4e02054ca80b41e4b30a512c3aee4981": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6162696dd7414b67a929955058c8807e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "634b8a6242e047569015b6d4bd037e88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "641e23c914604b36a4c569e9db835846": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bb573e75a5a46df976d3281b9ab708e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730397835bf045928b7fe743ea860fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf46acb645c14d1a80cd492a2b2db829",
            "placeholder": "​",
            "style": "IPY_MODEL_3eabef59e4174f3bb0ee5c1fd1ac644e",
            "value": " 1.16M/1.16M [00:00&lt;00:00, 1.79MB/s]"
          }
        },
        "76226fd3aee34b37a40d5b80df9c0542": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782a50cbddf4431086856736dd7e06e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a80e5e8e2c4f39ab28e863ef19cf94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e72256682d04056bc216cc866a45c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "978d1128d8a04343ad13af32d12f097c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3a180c9cc1844f6b89dce9e63d042915",
            "style": "IPY_MODEL_0d6f791d9d5840a7ab247c1e416adb21",
            "tooltip": ""
          }
        },
        "9a59159e7a104f5e91d4c97937382dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a3eb0fb92b48c89219bf788d0d6fe9",
            "placeholder": "​",
            "style": "IPY_MODEL_000c50171bd64d72903fcfc235666476",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "9cc10974a45949a9bb974a4c70ceeb62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "afd764d7fefe455cb7d9d63e1cc74a99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87199fc633f49febccbac9fbe9c8429": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d07382b5e84d42a8a23ad99a226635ff",
            "placeholder": "​",
            "style": "IPY_MODEL_6162696dd7414b67a929955058c8807e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bf46acb645c14d1a80cd492a2b2db829": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02698d024f5461bb508bf7795fdc83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_9cc10974a45949a9bb974a4c70ceeb62"
          }
        },
        "d07382b5e84d42a8a23ad99a226635ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81dab9de6b8483c9ea3843b77f51a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da2c6631722b45cc8bde8cdcda85c569": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_afd764d7fefe455cb7d9d63e1cc74a99",
            "style": "IPY_MODEL_fd2b378dfc1842c4863a7b4e1c0b3b6a",
            "value": true
          }
        },
        "e2a3eb0fb92b48c89219bf788d0d6fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8517cbc74f34b3a94e3e7b542436c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1add11fb1a4e00beca34bc04cc099d",
            "placeholder": "​",
            "style": "IPY_MODEL_8e72256682d04056bc216cc866a45c96",
            "value": "Connecting..."
          }
        },
        "ef1add11fb1a4e00beca34bc04cc099d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f38a5519367e48c2bfcb4347b4634a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a1932a9834045e1ace28edd57cc5de7",
              "IPY_MODEL_47e6f54fe03c4975a85f46eb0149e30a",
              "IPY_MODEL_730397835bf045928b7fe743ea860fb7"
            ],
            "layout": "IPY_MODEL_86a80e5e8e2c4f39ab28e863ef19cf94"
          }
        },
        "fa899221c1724c24aa1475954617f6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2b378dfc1842c4863a7b4e1c0b3b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe157cb587e041cc8e8f5f07b7a76936": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b87199fc633f49febccbac9fbe9c8429",
              "IPY_MODEL_0040d9da53f142c997494fd201d10f3c",
              "IPY_MODEL_28c1589c4e6043fd86d4658451e4daeb"
            ],
            "layout": "IPY_MODEL_03af1481ba0f4f24a425c248444cc111"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
